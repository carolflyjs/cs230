{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Processing_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1z0FybKzkPPz_-41hIlc4otEOL3E1F6Oi",
      "authorship_tag": "ABX9TyNdSoSbEdQXAd85OprV7B/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carolflyjs/cs230/blob/master/Data_Processing_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fnmjCAWx3D-",
        "colab_type": "code",
        "outputId": "93af2fa6-69e3-4833-aa33-c5bfe2f0116b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install pyspellchecker "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspellchecker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/d1/ec4e830e9f9c1fd788e1459dd09279fdf807bc7a475579fd7192450b879c/pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FReb7X4WxPaF",
        "colab_type": "code",
        "outputId": "f35c9806-ea7a-4b7f-ad36-55473de42a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from spellchecker import SpellChecker\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, concatenate, Dot, Embedding, LSTM, GRU\n",
        "from keras.engine import Layer\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WDtWGMhxuB_",
        "colab_type": "code",
        "outputId": "124ebf61-d8b7-4679-d693-8265f8662b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "class_dict = {\n",
        "  \"BT\": 0,\n",
        "  \"NT\": 1,\n",
        "  \"NPT\": 2,\n",
        "  \"PT\": 3,\n",
        "  \"URT\": 4\n",
        "}\n",
        "\n",
        "reverse_class_dict = {\n",
        "    0: \"BT\",\n",
        "    1: \"NT\",\n",
        "    2: \"NPT\",\n",
        "    3: \"PT\",\n",
        "    4: \"URT\"\n",
        "}\n",
        "\n",
        "def load_file(path, delimiter=\",\"):\n",
        "    df = pd.read_csv(path, delimiter=delimiter)\n",
        "    df.columns = [\"source\", \"target\", \"label\"]\n",
        "    df = shuffle(df).reset_index(drop=True)\n",
        "    class_dict = {\n",
        "      \"BT\": 0,\n",
        "      \"NT\": 1,\n",
        "      \"NPT\": 2,\n",
        "      \"PT\": 3,\n",
        "      \"URT\": 4\n",
        "    }\n",
        "    df[\"label\"] = df[\"label\"].apply(lambda x: class_dict[x])\n",
        "    return df\n",
        "\n",
        "def data_prep(df, x_columns, y_columns):\n",
        "    X_train = df[x_columns]\n",
        "    Y_train = df[y_columns]\n",
        "    Y_train = keras.utils.to_categorical(Y_train, num_classes=5)\n",
        "    return X_train, Y_train\n",
        "\n",
        "print('load_file(path, delimiter=\",\"): return df')\n",
        "print('data_prep(df, x_columns, y_columns): return X_train, Y_train')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load_file(path, delimiter=\",\"): return df\n",
            "data_prep(df, x_columns, y_columns): return X_train, Y_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV1VUJ_D0E5i",
        "colab_type": "code",
        "outputId": "665e57cb-dd8f-43e7-d6d5-b9d6399c44bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def removeSpellingErrors(df):\n",
        "    new_list = []\n",
        "    spell = SpellChecker()\n",
        "    raw_df_list = df.values.tolist()\n",
        "\n",
        "    for (source, target, label) in raw_df_list:\n",
        "        if len(spell.unknown(source.split())) == 0 and len(spell.unknown(target.split())) == 0:\n",
        "            new_list.append((source, target, label))\n",
        "    new_df = pd.DataFrame(new_list, columns=[\"source\", \"target\", \"label\"])\n",
        "    return new_df\n",
        "\n",
        "print('removeSpellingErrors(df): return new_df')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "removeSpellingErrors(df): return new_df\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3bj6YtZNf5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(X_list):\n",
        "    t = text.Tokenizer()\n",
        "    fitter_container = []\n",
        "    for X_data in X_list:\n",
        "      fitter_container.append(X_data[\"source\"])\n",
        "      fitter_container.append(X_data[\"target\"])\n",
        "    fit_text = pd.concat(fitter_container)\n",
        "    t.fit_on_texts(fit_text)\n",
        "    \n",
        "    transformed_container = []\n",
        "    for X_data in X_list:\n",
        "      X_transformed = X_data.copy() \n",
        "      X_transformed[\"source\"] = t.texts_to_sequences(X_data[\"source\"])\n",
        "      X_transformed[\"target\"] = t.texts_to_sequences(X_data[\"target\"])\n",
        "      transformed_container.append(X_transformed)\n",
        "\n",
        "    max_value = 0\n",
        "    for X_data in transformed_container:\n",
        "      for series in [\"source\", \"target\"]:\n",
        "          current_max = (X_data[series].apply(lambda ls: len(ls))).max()\n",
        "          if current_max > max_value:\n",
        "              max_value = current_max\n",
        "    max_value = max_value\n",
        "    vocab_size = len(t.index_word) + 1\n",
        "    return transformed_container, max_value, vocab_size, t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMVD6OEKNjBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad(X, max_value):\n",
        "    source = sequence.pad_sequences(X[\"source\"],  maxlen=max_value)\n",
        "    target = sequence.pad_sequences(X[\"target\"],  maxlen=max_value)\n",
        "    return pd.DataFrame(data=np.concatenate((source, target), axis=1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R5RE65105BC",
        "colab_type": "code",
        "outputId": "e482d29d-d244-45a0-cb55-c3bfccbc30ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def tokenize_and_pad(X_train):\n",
        "\n",
        "\n",
        "    X_train_tokenized, max_value, vocab_size, tokenizer = tokenize(X_train)\n",
        "    X_train_padded = pad(X_train_tokenized, max_value=max_value)\n",
        "    return X_train_padded, max_value, vocab_size, tokenizer\n",
        "\n",
        "print('tokenize_and_pad(X_train), return X_train_padded, max_value, vocab_size, tokenizer')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenize_and_pad(X_train), return X_train_padded, max_value, vocab_size, tokenizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xghoorp05nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "container = list()\n",
        "\n",
        "df_train = load_file(\"/content/drive/My Drive/cs230/train.csv\")\n",
        "X_train_raw, Y_train = data_prep(df_train, [\"source\", \"target\"], [\"label\"])\n",
        "\n",
        "df_dev = load_file(\"/content/drive/My Drive/cs230/dev.csv\")\n",
        "X_dev_raw, Y_dev = data_prep(df_dev, [\"source\", \"target\"], [\"label\"])\n",
        "\n",
        "df_test = load_file(\"/content/drive/My Drive/cs230/test.csv\")\n",
        "X_test_raw, Y_test = data_prep(df_test, [\"source\", \"target\"], [\"label\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yweTgeAGWI_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_list, max_value, vocab_size, tokenizer = tokenize([df_train, df_dev, df_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp1vo0mwV5XM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_tokenized = pad(tokenized_list[0], max_value)\n",
        "X_dev_tokenized = pad(tokenized_list[1], max_value)\n",
        "X_test_tokenized = pad(tokenized_list[2], max_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3NaLEEW1g4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weight = {0: 1, 1: 1, 2: 5, 3: 5, 4: 2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVKHPedwotJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}